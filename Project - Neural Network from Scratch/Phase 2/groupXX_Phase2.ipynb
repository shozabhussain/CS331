{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "groupXX_Phase2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYT-2x47-sjY"
      },
      "source": [
        "# CS331 - Spring 2021 - Phase 2 [10%]\n",
        "\n",
        "*__Submission Guidelines:__*\n",
        "- Naming convention for submission of this notebook is `groupXX_Phase2.ipynb` where XX needs to be replaced by your group number. For example: group 1 would rename their notebook to `group01_Phase2.ipynb`\n",
        "- Only the group lead is supposed to make the submission\n",
        "- Only the single .ipynb file needs to be submitted via LMS \n",
        "- All the cells <b>must</b> be run once before submission. If your submission's cells are not showing the results (plots etc.), marks wil be deducted\n",
        "- Only the code written within this notebook will be considered while grading. No other files will be entertained\n",
        "- You are advised to follow good programming practies including approriate variable naming and making use of logical comments \n",
        "\n",
        "The university honor code should be maintained. Any violation, if found, will result in disciplinary action. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpC12Mye-9uZ"
      },
      "source": [
        "#### <b>Introduction</b> \n",
        "This is the second of the three phases of this offering's project. Having implemented a neural network from scratch using for loops, you will now be using vectorization (wherever applicable) to improve the performance of the model and equip it to deal with the complete Fashion MNIST Dataset.\n",
        "\n",
        "The dataset consists of 70,000 images of fashion/clothing items belonging to 10 different categories/classes. It has furhter been divided into 60,000 training images and 10,000 test images and each image is a 28*28 grayscale image (hence 1 color channel). It is recommended that you go through  [this link](https://www.kaggle.com/zalando-research/fashionmnist) to familiarize yourself with the dataset.\n",
        "\n",
        "In this phase, you will load the required dataset from the keras datasets library. To get the best results, you will need to tweak hyperparameters, and the best values will not necessarily be the ones you performed for your computations in phase 1 with.\n",
        "\n",
        "###### <b>IMPORTANT\n",
        "\n",
        "In this phase, you will be required to implement multiple hidden layers (>= 2) alongside the input and output layer, as opposed to the one you implemented in phase 1. This may require some revision to the logic of the code you use.\n",
        "\n",
        "###### Modification of the provided code without prior discussion with the TAs will result in a grade deduction</b>\n",
        "\n",
        "---\n",
        "\n",
        "###### <b>Side note</b>\n",
        "The `plot_model` method will only work if you have the `pydot` python package installed along with [Graphviz](https://graphviz.gitlab.io/download/). If you do not wish to use this then simply comment out the import for `pydot`\n",
        "\n",
        "###### <b>Need Help?</b>\n",
        "If you need help, please refer to the course staff ASAP and do not wait till the last moment as they might not be available on very short notice close to deadlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idDbgABDBuvZ"
      },
      "source": [
        "#### <b>Before You Begin</b>\n",
        "\n",
        "Skeleton code is provided to get you started. The main methods that you need to implement correspond to the four steps of the training process of a NN which are as follows:\n",
        "1. Initialize variables and initialize weights\n",
        "2. Forward pass\n",
        "3. Backward pass AKA Backpropogation\n",
        "4. Weight Update AKA Gradient Descent\n",
        "\n",
        "__Look for comments in the code to see where you are supposed to write your code__ \n",
        "\n",
        "A `fit` function is what combines the previous three functions and overall trains the network to __fit__ to the provided training examples. The provided `fit` methods requires all the four steps of the training process to be working correctly. The function has been setup in a way that it expects the above four methods to take particular inputs and return particular outputs. __You are supposed to work within this restriction__ \n",
        "\n",
        "\n",
        "\n",
        "__To see if your model is working correctly, you need to make sure that your model loss is going down during training__\n",
        "\n",
        "__The number of hidden layers needs to be >= 2, and the implementation should not crash if we attempt to arbitrarily change it__\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0OE_p0V-q-H"
      },
      "source": [
        "# making all the necessary imports here\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "from IPython.display import Image\n",
        "import pydot\n",
        "from tqdm import tqdm_notebook\n",
        "import seaborn as sns\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz6xWsK3B-k4"
      },
      "source": [
        "# This function will be used to plot the confusion matrix at the end of this notebook\n",
        "\n",
        "def plot_confusion_matrix(conf_mat):\n",
        "    classes = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "    df_cm = pd.DataFrame(conf_mat,classes,classes)\n",
        "    plt.figure(figsize=(15,9))\n",
        "    sns.set(font_scale=1.4)\n",
        "    sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})\n",
        "    plt.show()\n",
        "\n",
        "class_labels = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8vdVLQ0B_bd"
      },
      "source": [
        "# Enter group lead's roll number here. This will be used for plotting purposes\n",
        "\n",
        "rollnumber = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK2xKOxzCqgE"
      },
      "source": [
        "#### __Dataset from Keras Library__\n",
        "\n",
        "The required library has been imported for you as fashion_mnist. Use it to load the train and test data accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ehOJSGECqOD"
      },
      "source": [
        "classes = 10 # Do not change this\n",
        "\n",
        "# Download Fashion MNIST dataset\n",
        "###### Code Here ######\n",
        "\n",
        "# Split the fashion MNIST dataset into train and test sets\n",
        "# Convert y_train and y_test to categorical binary values \n",
        "###### Code Here ######\n",
        "\n",
        "#Reshape train and test images as one-dimensional arrays\n",
        "###### Code Here ######\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLeZFElYFINX"
      },
      "source": [
        "#### __NN Implementation__\n",
        "Your implementation of NN needs to use the `sigmoid` activation function for all hidden layers and the `softmax` activation function for the output layer. The NN model you will be creating here will consits of only three layers: 1 input layer, n hidden layers (where you have the liberty to define n) and 1 output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVq8Oq-MD2wZ"
      },
      "source": [
        "class NeuralNetwork():\n",
        "    @staticmethod\n",
        "    def cross_entropy_loss(y_pred, y_true):\n",
        "        # implement cross_entropy_loss function\n",
        "        #TO DO\n",
        "\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(y_pred, y_true):\n",
        "        # implement accuracy function\n",
        "        #TO DO\n",
        "\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        # implement softmax function\n",
        "        #TO DO\n",
        "\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        #TO DO\n",
        "        \n",
        "        return None\n",
        "\n",
        "    def __init__(self, nodes_per_layer):\n",
        "        '''Creates a Feed-Forward Neural Network.\n",
        "        The parameters represent the number of nodes in each layer. \n",
        "        Look at the inputs to the function, and use 'try and accept'\n",
        "        to catch errors if number of layers are < 2.\n",
        "        '''\n",
        "        \n",
        "        self.num_layers = None # including input and output layers\n",
        "        self.nodes_per_layer = None\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "\n",
        "        self.weights_ = []\n",
        "        self.biases_ = []\n",
        "        self.__init_weights(nodes_per_layer)\n",
        "\n",
        "    def __init_weights(self, nodes_per_layer):\n",
        "        '''Initializes all weights based on standard normal distribution and all biases to 0.'''\n",
        "        '''Initialize weights for each layer except the input layer, since it does not have weights.'''\n",
        "        \n",
        "        ###### Code Here ######\n",
        "       \n",
        "    \n",
        "    def forward_pass(self, input_data):\n",
        "        '''Executes the feed forward algorithm.\n",
        "        \"input_data\" is the input to the network in row-major form\n",
        "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
        "        \n",
        "        ###### Code Here ######\n",
        "        activations =[]\n",
        "     \n",
        "        return activations\n",
        "\n",
        "    def backward_pass(self, targets, layer_activations):\n",
        "        '''Executes the backpropogation algorithm.\n",
        "        \"targets\" is the ground truth/labels\n",
        "        \"layer_activations\" are the return value of the forward pass step\n",
        "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course)'''\n",
        "        \n",
        "        ###### Code Here ######\n",
        "        deltas = []\n",
        "        \n",
        "        return deltas\n",
        "    \n",
        "    def weight_update(self, deltas, layer_inputs, lr):\n",
        "        '''Executes the gradient descent algorithm.\n",
        "        \"deltas\" is return value of the backward pass step\n",
        "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
        "        \"lr\" is the learning rate'''\n",
        "        \n",
        "        ###### Code Here ######\n",
        "  \n",
        "    \n",
        "    ###### Do Not Change Anything Below this line in This Cell ######\n",
        "    \n",
        "    def fit(self, Xs, Ys, epochs, lr=1e-3):\n",
        "            history = []\n",
        "            for epoch in tqdm_notebook(range(epochs)):\n",
        "                num_samples = Xs.shape[0]\n",
        "                for i in range(num_samples):\n",
        "\n",
        "                    sample_input = Xs[i,:].reshape((1,self.input_shape))\n",
        "                    sample_target = Ys[i,:].reshape((1,self.output_shape))\n",
        "                    \n",
        "                    activations = self.forward_pass(sample_input)   # Call forward_pass function \n",
        "                    deltas = self.backward_pass(sample_target, activations)    # Call backward_pass function \n",
        "                    layer_inputs = [sample_input] + activations[:-1]\n",
        "                    \n",
        "                    # Call weight_update function \n",
        "                    self.weight_update(deltas, layer_inputs, lr)\n",
        "                \n",
        "                preds = self.predict(Xs)   # Call predict function \n",
        "\n",
        "                current_loss = self.cross_entropy_loss(preds, Ys)\n",
        "                \n",
        "                if  epoch==epochs-1:\n",
        "                  confusion_mat=confusion_matrix(Ys.argmax(axis=1), preds.argmax(axis=1),labels=np.arange(10))  \n",
        "                  plot_confusion_matrix(confusion_mat)\n",
        "                  report = classification_report(Ys, np_utils.to_categorical(preds.argmax(axis=1),num_classes=classes), target_names=class_labels)\n",
        "                  print(report)\n",
        "                history.append(current_loss)\n",
        "            return history\n",
        "    \n",
        "    def predict(self, Xs):\n",
        "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
        "        predictions = []\n",
        "        num_samples = Xs.shape[0]\n",
        "        for i in range(num_samples):\n",
        "            sample = Xs[i,:].reshape((1,self.input_shape))\n",
        "            sample_prediction = self.forward_pass(sample)[-1]\n",
        "            predictions.append(sample_prediction.reshape((self.output_shape,)))\n",
        "        return np.array(predictions)\n",
        "    \n",
        "    def evaluate(self, Xs, Ys):\n",
        "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
        "        pred = self.predict(Xs)\n",
        "        return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))\n",
        "    \n",
        "    def plot_model(self, filename):\n",
        "        '''Provide the \"filename\" as a string including file extension. Creates an image showing the model as a graph.'''\n",
        "        graph = pydot.Dot(graph_type='digraph')\n",
        "        graph.set_rankdir('LR')\n",
        "        graph.set_node_defaults(shape='circle', fontsize=0)\n",
        "        nodes_per_layer = [self.input_shape, self.hidden_shape, self.output_shape]\n",
        "        for i in range(self.num_layers-1):\n",
        "            for n1 in range(nodes_per_layer[i]):\n",
        "                for n2 in range(nodes_per_layer[i+1]):\n",
        "                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')\n",
        "                    graph.add_edge(edge)\n",
        "        graph.write_png(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew6nN0HyfbSd"
      },
      "source": [
        "# These are what we call the hyperparameters (a.k.a Black Magic). You need to research on them and tweak them to see what generates the best result for you \n",
        "\n",
        "EPOCH = None            # must be an int\n",
        "LEARNING_RATE = None\n",
        "nodes_per_layer = []  #int values for nodes of each layer. # of hidden layers >= 2. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6TTr8h1fo4l"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "# Instantiate the neural network with the number of nodes you choose per layer, right now it is done for three layers only.\n",
        "nn = NeuralNetwork(nodes_per_layer=nodes_per_layer)\n",
        "history = nn.fit(trainX, trainy, epochs=EPOCH, lr=LEARNING_RATE)\n",
        "plt.plot(history);\n",
        "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title='Training Plot {}'.format(rollnumber));\n",
        "end = time.time()\n",
        "\n",
        "print(\"Runtime of the algorithm is \", round((end - start),3),\" seconds\")\n",
        "\n",
        "# print accuracy on test set here\n",
        "print(nn.evaluate(None,None))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}